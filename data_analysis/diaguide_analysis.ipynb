{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# DiaGuide LLM Analysis\n",
    "\n",
    "## Comparative Evaluation of Large Language Models and Healthcare Professionals in Diabetes Guidance\n",
    "\n",
    "This notebook reproduces the results and figures from the study comparing GPT-4o responses with healthcare professional responses to diabetes-related questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import chi2_contingency, mannwhitneyu, kruskal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set the style for the plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 8),\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ratings_v310125.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Convert columns to appropriate types if needed\n",
    "# Convert binary columns to boolean\n",
    "df['is_healthcare_personnel'] = df['is_healthcare_personnel'].astype(bool)\n",
    "df['previous_participation'] = df['previous_participation'].astype(bool)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "### 3.1 Dataset Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique participants, questions, and answers\n",
    "unique_participants = df['user_id'].nunique()\n",
    "unique_questions = df['question_id'].nunique()\n",
    "unique_answers = df['answer_id'].nunique()\n",
    "\n",
    "print(f\"\\nUnique participants: {unique_participants}\")\n",
    "print(f\"Unique questions: {unique_questions}\")\n",
    "print(f\"Unique answers: {unique_answers}\")\n",
    "\n",
    "# Count LLM vs Human responses\n",
    "df['source'] = df['source'].str.lower()  # Ensure consistency in source labels\n",
    "llm_responses = df[df['source'] == 'llm'].shape[0]\n",
    "human_responses = df[df['source'] == 'human'].shape[0]\n",
    "\n",
    "print(f\"\\nLLM responses: {llm_responses}\")\n",
    "print(f\"Human responses: {human_responses}\")\n",
    "print(f\"Total ratings: {df.shape[0]}\")\n",
    "\n",
    "# Calculate average ratings per user\n",
    "ratings_per_user = df.groupby('user_id').size()\n",
    "avg_ratings_per_user = ratings_per_user.mean()\n",
    "median_ratings_per_user = ratings_per_user.median()\n",
    "\n",
    "print(f\"\\nAverage ratings per user: {avg_ratings_per_user:.1f}\")\n",
    "print(f\"Median ratings per user: {median_ratings_per_user}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Participant Demographics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demographic_table():\n",
    "    # Participant groups\n",
    "    groups = {\n",
    "        'Diabetes and Healthcare Professional': df[(df['has_diabetes'] == 'yes') & \n",
    "                                                 (df['is_healthcare_personnel'] == True)],\n",
    "        'Only Diabetes': df[(df['has_diabetes'] == 'yes') & \n",
    "                           (df['is_healthcare_personnel'] == False)],\n",
    "        'Only Healthcare Professional': df[(df['has_diabetes'] == 'no') & \n",
    "                                         (df['is_healthcare_personnel'] == True)],\n",
    "        'Neither': df[(df['has_diabetes'] == 'no') & \n",
    "                     (df['is_healthcare_personnel'] == False)]\n",
    "    }\n",
    "    \n",
    "    demographic_data = []\n",
    "    \n",
    "    # Count unique users in each group\n",
    "    for group_name, group_df in groups.items():\n",
    "        unique_users = group_df['user_id'].nunique()\n",
    "        ratings_count = group_df.shape[0]\n",
    "        demographic_data.append({\n",
    "            'Characteristic': 'Participant and Groups',\n",
    "            'Subgroup': group_name,\n",
    "            'Frequency (N)': unique_users,\n",
    "            'Ratings': ratings_count\n",
    "        })\n",
    "    \n",
    "    # Age groups\n",
    "    age_groups = df.groupby('age_group')\n",
    "    for age, group in age_groups:\n",
    "        demographic_data.append({\n",
    "            'Characteristic': 'Age Group',\n",
    "            'Subgroup': age,\n",
    "            'Frequency (N)': group['user_id'].nunique(),\n",
    "            'Ratings': group.shape[0]\n",
    "        })\n",
    "        \n",
    "    # Gender\n",
    "    gender_groups = df.groupby('gender')\n",
    "    for gender, group in gender_groups:\n",
    "        demographic_data.append({\n",
    "            'Characteristic': 'Gender',\n",
    "            'Subgroup': gender,\n",
    "            'Frequency (N)': group['user_id'].nunique(),\n",
    "            'Ratings': group.shape[0]\n",
    "        })\n",
    "        \n",
    "    # Education Level\n",
    "    edu_groups = df.groupby('education_level')\n",
    "    for edu, group in edu_groups:\n",
    "        demographic_data.append({\n",
    "            'Characteristic': 'Education Level',\n",
    "            'Subgroup': edu,\n",
    "            'Frequency (N)': group['user_id'].nunique(),\n",
    "            'Ratings': group.shape[0]\n",
    "        })\n",
    "        \n",
    "    # Healthcare Professional Type\n",
    "    hcp_groups = df[df['is_healthcare_personnel']].groupby('healthcare_professional_type')\n",
    "    for hcp_type, group in hcp_groups:\n",
    "        if pd.notna(hcp_type):  # Skip NaN values\n",
    "            demographic_data.append({\n",
    "                'Characteristic': 'Healthcare Professional Type',\n",
    "                'Subgroup': hcp_type,\n",
    "                'Frequency (N)': group['user_id'].nunique(),\n",
    "                'Ratings': group.shape[0]\n",
    "            })\n",
    "    \n",
    "    # Create a DataFrame for better display\n",
    "    demographic_table = pd.DataFrame(demographic_data)\n",
    "    \n",
    "    return demographic_table\n",
    "\n",
    "# Generate and display the demographic table\n",
    "demographic_table = generate_demographic_table()\n",
    "demographic_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats_by_source():\n",
    "    metrics = ['knowledge', 'helpfulness', 'empathy']\n",
    "    stats_data = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for source in ['llm', 'human']:\n",
    "            source_data = df[df['source'] == source][metric]\n",
    "            \n",
    "            stats_data.append({\n",
    "                'Metric': metric.capitalize(),\n",
    "                'Source': source.upper(),\n",
    "                'n': len(source_data),\n",
    "                'Mean': source_data.mean(),\n",
    "                'SD': source_data.std(),\n",
    "                '95% CI Lower': source_data.mean() - 1.96 * (source_data.std() / np.sqrt(len(source_data))),\n",
    "                '95% CI Upper': source_data.mean() + 1.96 * (source_data.std() / np.sqrt(len(source_data))),\n",
    "                'Median': source_data.median(),\n",
    "                'Min': source_data.min(),\n",
    "                'Max': source_data.max()\n",
    "            })\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_data)\n",
    "    stats_df['95% CI'] = stats_df.apply(lambda x: f\"[{x['95% CI Lower']:.2f}, {x['95% CI Upper']:.2f}]\", axis=1)\n",
    "    \n",
    "    # Reformat the dataframe to match the paper's table\n",
    "    formatted_stats = stats_df[['Metric', 'Source', 'n', 'Mean', 'SD', '95% CI', 'Median']]\n",
    "    formatted_stats['Mean (SD)'] = formatted_stats.apply(lambda x: f\"{x['Mean']:.2f} ({x['SD']:.2f})\", axis=1)\n",
    "    \n",
    "    return formatted_stats[['Metric', 'Source', 'n', 'Mean (SD)', '95% CI', 'Median']]\n",
    "\n",
    "# Generate descriptive statistics table\n",
    "desc_stats_table = descriptive_stats_by_source()\n",
    "desc_stats_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
